{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCvXt86iRPEjdN56ciWJK2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ue0BGUVYV5KG","executionInfo":{"status":"ok","timestamp":1678860561741,"user_tz":-330,"elapsed":39182,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}},"outputId":"9c0f2199-89b2-47c0-8f7e-621e235fb6fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=45f816147aaad52700bed6566228096e54d19c9e80715a57917e5348389be031\n","  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark=(SparkSession.builder.appName(\"...\").getOrCreate())"],"metadata":{"id":"fH3Bt-PDWSqu","executionInfo":{"status":"ok","timestamp":1678860571048,"user_tz":-330,"elapsed":9311,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["sc=spark.sparkContext\n","sqlContext=spark"],"metadata":{"id":"lZFVxTwzWvE0","executionInfo":{"status":"ok","timestamp":1678860571049,"user_tz":-330,"elapsed":15,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import pyspark.ml"],"metadata":{"id":"9k_H0AdiW2zk","executionInfo":{"status":"ok","timestamp":1678860571049,"user_tz":-330,"elapsed":14,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import Binarizer"],"metadata":{"id":"DNRrZxe3W8cM","executionInfo":{"status":"ok","timestamp":1678860571713,"user_tz":-330,"elapsed":678,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.readwriter import DataFrameWriterV2\n","#Data Types\n","#Local vector\n","import numpy as np\n","import scipy.sparse as sps\n","from pyspark.mllib.linalg import Vectors\n","#Use a Numpy array as a dense vector.\n","dv1=np.array([1.0,0.0,3.0])\n","dv1\n","#Use a Python list as a dense vector\n","dv2=[1.0,0.0,3.0]\n","DataFrameWriterV2\n","#Create a SparseVector.\n","sv1=Vectors.sparse(3,[0,2],[1.0,3.0])\n","sv1\n","#Use a single-column Scipy csc_matrix as a sparese vector.\n","sv2=sps.csc_matrix((np.array([1.0,3.0]), np.array([0,2]), np.array([0,2])),shape=(3,1))\n","sv2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJkMCL__XHLN","executionInfo":{"status":"ok","timestamp":1678860571713,"user_tz":-330,"elapsed":4,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}},"outputId":"13f7473c-60a9-49ef-87ec-beae82633a9f"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<3x1 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 2 stored elements in Compressed Sparse Column format>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["#Labled point\n","from pyspark.mllib.linalg import SparseVector\n","from pyspark.mllib.regression import LabeledPoint\n","\n","#Create a labeled point with a positive label and a dense feature  vector.\n","pos=LabeledPoint(1.0,[1.0,0.0,3.0])\n","\n","#Create a labeled point with a negative label and a dense feature  vector.\n","neg=LabeledPoint(0.0,SparseVector(3,[0,2],[1.0,3.0]))"],"metadata":{"id":"Um_X0prfXgwM","executionInfo":{"status":"ok","timestamp":1678860571713,"user_tz":-330,"elapsed":3,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","from pyspark.sql import Row\n","spark=SparkSession(sc)\n","allTypes=sc.parallelize([Row(i=1,s=\"string\",d=1.0,l=1,\n","b=True, list=[1,2,3], dict={\"s\":0}, row=Row(a=1),\n","time=datetime(2014,8,1,14,1,5))])\n","df=allTypes.toDF()\n","df.createOrReplaceTempView(\"allTypes\")\n","spark.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n","           'from allTypes where b and i > 0').collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRDeayCEdOOd","executionInfo":{"status":"ok","timestamp":1678860583493,"user_tz":-330,"elapsed":11783,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}},"outputId":"3b0eb1dc-3f10-4973-81bb-35cb346f8f8f"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2, dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["\n","from pyspark.sql.functions import col, count, lit, max\n","from pyspark.sql import Observation\n","df=spark.createDataFrame([[\"Alice\", 2], [\"Bob\", 5]], [\"name\",\"age\"])\n","observation=Observation(\"my metrics\")\n","observed_df=df.observe(observation, count(lit(1)).alias(\"count\"),max(col(\"age\")))\n","observed_df.count()\n","2\n","observation.get\n","{'count':2, 'max(age)':5}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDzlK_F4fEOy","executionInfo":{"status":"ok","timestamp":1678860585699,"user_tz":-330,"elapsed":2208,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}},"outputId":"b499aff3-0e34-4345-f256-4746ad86ca7b"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'count': 2, 'max(age)': 5}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["row=Row(name=\"Alice\", age=11)\n","row\n","Row(name='Alice',age=11)\n","row['name'],row['age']\n","('Alice',11)\n","row.name, row.age\n","('Alice',11)\n","'name' in row\n","True\n","'wrong_key' in row"],"metadata":{"id":"dwS59D0GrGdJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678860585699,"user_tz":-330,"elapsed":3,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}},"outputId":"06158c61-6a88-449d-8bd2-5dfe857353af"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\n","from pyspark.ml.stat import Correlation\n","\n","data =[(Vectors.sparse(4,[(0,1.0),(3,-2.0)]),),\n","       (Vectors.dense([4.0,5.0,0.0,3.0]),),\n","       (Vectors.dense([6.0,7.0,0.0,8.0]),),\n","       (Vectors.sparse(4,[(0,1.0),(3,-2.0)]),),]\n","df=spark.createDataFrame(data,[\"features\"])\n","r1=Correlation.corr(df,\"features\").head()\n","print(\"Pearson correlation matrix:\\n\"+str(r1[0]))\n","r2=Correlation.corr(df,\"features\",\"spearman\").head()\n","print(\"Spearman correlation matrix:\\n\"+str(r2[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZfn2TC5nUsn","executionInfo":{"status":"ok","timestamp":1678860591395,"user_tz":-330,"elapsed":5698,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}},"outputId":"969d3f7f-7b60-4411-c08f-5c0ff947f0a9"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Pearson correlation matrix:\n","DenseMatrix([[1.        , 0.99413485,        nan, 0.99493668],\n","             [0.99413485, 1.        ,        nan, 0.97823198],\n","             [       nan,        nan, 1.        ,        nan],\n","             [0.99493668, 0.97823198,        nan, 1.        ]])\n","Spearman correlation matrix:\n","DenseMatrix([[ 1.,  1., nan,  1.],\n","             [ 1.,  1., nan,  1.],\n","             [nan, nan,  1., nan],\n","             [ 1.,  1., nan,  1.]])\n"]}]},{"cell_type":"code","source":["from pyspark.ml.feature import FeatureHasher\n","dataset=spark.createDataFrame([\n","    (2.2, True, \"1\", \"foo\"),\n","    (3.3,False,\"2\",\"bar\"),\n","    (4.4,False,\"3\",\"baz\"),\n","    (5.5,False,\"4\",\"foo\")\n","  \n","],[\"real\",\"bool\",\"stringNum\",\"string\"])\n","\n","hasher=FeatureHasher(inputCols=[\"real\",\"bool\",\"stringNum\",\"string\"],\n","                     outputCol=\"features\")\n","featurized=hasher.transform(dataset)\n","featurized.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nR-08Yw8nXIv","executionInfo":{"status":"ok","timestamp":1678860593891,"user_tz":-330,"elapsed":2508,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}},"outputId":"1bffef8f-c1bc-4178-dc2f-b2ca575a2018"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+-----+---------+------+--------------------------------------------------------+\n","|real|bool |stringNum|string|features                                                |\n","+----+-----+---------+------+--------------------------------------------------------+\n","|2.2 |true |1        |foo   |(262144,[174475,247670,257907,262126],[2.2,1.0,1.0,1.0])|\n","|3.3 |false|2        |bar   |(262144,[70644,89673,173866,174475],[1.0,1.0,1.0,3.3])  |\n","|4.4 |false|3        |baz   |(262144,[22406,70644,174475,187923],[1.0,1.0,4.4,1.0])  |\n","|5.5 |false|4        |foo   |(262144,[70644,101499,174475,257907],[1.0,1.0,5.5,1.0]) |\n","+----+-----+---------+------+--------------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer, RegexTokenizer\n","from pyspark.sql.functions import col, udf\n","from pyspark.sql.types import IntegerType\n","\n","sentenceDataFrame=spark.createDataFrame([\n","    (0,\" Hi heared about spark\"),\n","    (1,\"I wish Java could use case classes\"),\n","    (2,\"Logistic, regression, models,are,meat\")\n","\n","],[\"id\",\"sentence\"])\n","\n","tokenizer=Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","\n","regexTokenizer=RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\",pattern=\"\\\\w\")\n","\n","countTokens=udf(lambda words: len(words), IntegerType())\n","\n","tokenized=tokenizer.transform(sentenceDataFrame)\n","tokenized.select(\"sentence\",\"words\")\\\n","    .withColumn(\"tokens\",countTokens(col(\"words\"))).show(truncate=False)\n","\n","regexTokenized=regexTokenizer.transform(sentenceDataFrame)\n","regexTokenized.select(\"sentence\",\"words\")\\\n","    .withColumn(\"tokens\",countTokens(col(\"words\"))).show(truncate=False)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4zqefEVqEGV","executionInfo":{"status":"ok","timestamp":1678860598417,"user_tz":-330,"elapsed":4529,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}},"outputId":"4ebf90e2-dfd4-4ab1-f259-d1d84d763670"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------------------------+------------------------------------------+------+\n","|sentence                             |words                                     |tokens|\n","+-------------------------------------+------------------------------------------+------+\n","| Hi heared about spark               |[, hi, heared, about, spark]              |5     |\n","|I wish Java could use case classes   |[i, wish, java, could, use, case, classes]|7     |\n","|Logistic, regression, models,are,meat|[logistic,, regression,, models,are,meat] |3     |\n","+-------------------------------------+------------------------------------------+------+\n","\n","+-------------------------------------+------------------+------+\n","|sentence                             |words             |tokens|\n","+-------------------------------------+------------------+------+\n","| Hi heared about spark               |[ ,  ,  ,  ]      |4     |\n","|I wish Java could use case classes   |[ ,  ,  ,  ,  ,  ]|6     |\n","|Logistic, regression, models,are,meat|[, , , , ,, ,]    |4     |\n","+-------------------------------------+------------------+------+\n","\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/apache/spark.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFT3kE-T3msV","executionInfo":{"status":"ok","timestamp":1678860519940,"user_tz":-330,"elapsed":61295,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}},"outputId":"84f2521f-25e5-4857-fce7-d5ca1b9e3453"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'spark'...\n","remote: Enumerating objects: 995403, done.\u001b[K\n","remote: Counting objects: 100% (622/622), done.\u001b[K\n","remote: Compressing objects: 100% (365/365), done.\u001b[K\n","remote: Total 995403 (delta 216), reused 420 (delta 123), pack-reused 994781\u001b[K\n","Receiving objects: 100% (995403/995403), 456.63 MiB | 18.12 MiB/s, done.\n","Resolving deltas: 100% (446598/446598), done.\n","Updating files: 100% (20273/20273), done.\n"]}]},{"cell_type":"code","source":["#Stan\n","from pyspark.ml.feature import StandardScaler\n","dataFrame=spark.read.format(\"libsvm\").load(\"/content/spark/data/mllib/sample_libsvm_data.txt\")\n","scaler=StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n","                      withStd=True, withMean=False)\n","#Compute summary statistics by fitting the standerscaler\n","scalerModel=scaler.fit(dataFrame)\n","\n","scaledData=scalerModel.transform(dataFrame)\n","scaledData.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AM-jzg2D32HI","executionInfo":{"status":"ok","timestamp":1678861390115,"user_tz":-330,"elapsed":1141,"user":{"displayName":"Avadhesh Giri","userId":"10450720448986358812"}},"outputId":"b4b3634b-643a-47a8-b5e0-69545bb12191"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+--------------------+--------------------+\n","|label|            features|      scaledFeatures|\n","+-----+--------------------+--------------------+\n","|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n","|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n","|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n","|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n","|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n","|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n","|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n","|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n","|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n","|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n","|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n","|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n","|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n","|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n","|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n","|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n","|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n","|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n","|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n","|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n","+-----+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}]}]}